{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYGYdlJG3Jyu",
        "outputId": "d917ade4-6d57-4111-bce7-7169ffd56ee3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Add your Google Drive.\n",
        "# NOTE: In my Google Drive. I have model(llama-2-7b-chat.ggmlv3.q8_0.bin) in my_model directory.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wC1zt394_iM"
      },
      "outputs": [],
      "source": [
        "!pip install langchain\n",
        "!pip install ctransformers\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "esvVeTHP3VhG"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import CTransformers\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vPWQBduG4biQ"
      },
      "outputs": [],
      "source": [
        "DATA_PATH='data/'                   # In this folder we have all pdf.\n",
        "CHUNK_SIZE=500\n",
        "CHUNK_OVERLAP=50\n",
        "DB_FAISS_PATH='embed_vector'        # In this folder we store embedded vector.\n",
        "\n",
        "# Build vector database\n",
        "def run_db_build():\n",
        "    loader = DirectoryLoader(DATA_PATH,\n",
        "                             glob='*.pdf',\n",
        "                             loader_cls=PyPDFLoader)\n",
        "    documents = loader.load()\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE,\n",
        "                                                   chunk_overlap=CHUNK_OVERLAP)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "\n",
        "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
        "                                       model_kwargs={'device': 'cpu'})\n",
        "\n",
        "    vectorstore = FAISS.from_documents(texts, embeddings)\n",
        "    vectorstore.save_local(DB_FAISS_PATH)\n",
        "\n",
        "run_db_build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z0RHBaVo41N8"
      },
      "outputs": [],
      "source": [
        "MODEL_BIN_PATH='/content/drive/MyDrive/my_model/llama-2-7b-chat.ggmlv3.q8_0.bin'\n",
        "MODEL_TYPE='llama'\n",
        "MAX_NEW_TOKENS=256\n",
        "TEMPERATURE=0.7\n",
        "VECTOR_COUNT=2\n",
        "RETURN_SOURCE_DOCUMENTS=False\n",
        "\n",
        "\n",
        "qa_template = \"\"\"Use the following pieces of information to answer the user's question.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\n",
        "Only return the helpful answer below and nothing else.\n",
        "Helpful answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fJ5IjTZw5xee"
      },
      "outputs": [],
      "source": [
        "def build_llm():\n",
        "    # Local CTransformers model\n",
        "    llm = CTransformers(model=MODEL_BIN_PATH,\n",
        "                        model_type=MODEL_TYPE,\n",
        "                        config={'max_new_tokens': MAX_NEW_TOKENS,\n",
        "                                'temperature': TEMPERATURE}\n",
        "                        )\n",
        "\n",
        "    return llm\n",
        "\n",
        "def set_qa_prompt():\n",
        "    \"\"\"\n",
        "    Prompt template for QA retrieval for each vectorstore\n",
        "    \"\"\"\n",
        "    prompt = PromptTemplate(template=qa_template,\n",
        "                            input_variables=['context', 'question'])\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def build_retrieval_qa(llm, prompt, vectordb):\n",
        "    dbqa = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                       chain_type='stuff',\n",
        "                                       retriever=vectordb.as_retriever(search_kwargs={'k': VECTOR_COUNT}),\n",
        "                                       return_source_documents=RETURN_SOURCE_DOCUMENTS,\n",
        "                                       chain_type_kwargs={'prompt': prompt}\n",
        "                                       )\n",
        "    return dbqa\n",
        "\n",
        "\n",
        "def setup_dbqa():\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                                       model_kwargs={'device': 'cpu'})\n",
        "    vectordb = FAISS.load_local(DB_FAISS_PATH, embeddings)\n",
        "    llm = build_llm()\n",
        "    qa_prompt = set_qa_prompt()\n",
        "    dbqa = build_retrieval_qa(llm, qa_prompt, vectordb)\n",
        "\n",
        "    return dbqa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PP9S2iVA5_Gk"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    query = input(\"Enter your query: \")\n",
        "    qa_result = setup_dbqa()\n",
        "    response = qa_result({'query': query})\n",
        "    print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZthwO-a6BWx",
        "outputId": "40555aed-48ac-4285-fe25-db354c37208f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query: who said God is in the details?\n",
            "{'query': 'who said God is in the details?', 'result': 'Ludwig Mies van der Rohe', 'source_documents': [Document(page_content='the kitchen sink. Most arts echo analogous sentiments. In our search for others whoascribe God’ s home as being in the details, we ﬁnd ourselves in the good company of the19th century French author Gustav Flaubert. The French poet Paul Valery advises us that apoem is never done and bears continual rework, and to stop working on it is abandonment.Such preoccupation with detail is common to all endeavors of excellence. So maybe thereis little new here, but in reading this book you will be', metadata={'source': 'data/test.pdf', 'page': 2}), Document(page_content='already wanted to say here. Small things matter. This is a book about humble concernswhose value is nonetheless far from small.\\nGod is in the details , said the architect Ludwig mies van der Rohe. This quote recalls', metadata={'source': 'data/test.pdf', 'page': 0})]}\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTM1zgBN6DE9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
